# -*- coding: utf-8 -*-
"""text-to-sql-modelFB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4WpZgS5hKxYvK1X5er0O3I2x0M3ELP9
"""

!pip install --quiet peft accelerate bitsandbytes trl transformers datasets torch gradio

!huggingface-cli login

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

from datasets import load_dataset

# Load your dataset (replace this with your specific dataset)
dataset = load_dataset('gretelai/synthetic_text_to_sql')

def preprocess_function(example):
    return {
        "text": f"### Instruction:\n{example['sql_prompt']}\n\n### Context:\n{example['sql_context']}\n\n### SQL:\n{example['sql']}"
    }



dataset = dataset.map(preprocess_function)
dataset

dataset['train']['text'][0]

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype="bfloat16",
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer.pad_token = tokenizer.eos_token

from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

def tokenize(example):
    tokens = tokenizer(
        example["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens


tokenized_datasets = dataset.map(tokenize, batched=True)

train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["test"]

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="/llama3-sql_full",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=1,
    learning_rate=2e-4,
    #max_steps=None,
    bf16=True,
    fp16=False,
    eval_steps = 100,
    save_strategy="epoch",
    logging_steps=20,
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

trainer.train()

model.save_pretrained("llama3-sql-finetuned_full")
tokenizer.save_pretrained("llama3-sql-finetuned_full")
trainer.save_model("llama3-sql-finetuned_full")

model.eval()
prompt = "List all customers who placed orders in the last 3 days in order"
inputs = tokenizer(f"### Instruction:\n{prompt}", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))



"""merge"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel


base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, "llama3-sql-finetuned_full")
model = model.merge_and_unload()
model.save_pretrained("llama3-sql-merged_full")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer.save_pretrained("llama3-sql-merged_full")
model.config.save_pretrained("llama3-sql-merged_full")

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("llama3-sql-merged_full")
tokenizer = AutoTokenizer.from_pretrained("llama3-sql-merged_full")

prompt = "Write a SQL query that lists all employees who joined in the last 30 days."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))





"""push to hub"""

from transformers import AutoModelForCausalLM, AutoTokenizer
model_name='/content/llama3-sql-merged_full'
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model.push_to_hub('ferdaous/mytexttosqlmodel3')
tokenizer.push_to_hub('ferdaous/mytexttosqlmodel3')



from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("ferdaous/mytexttosqlmodel3")
tokenizer = AutoTokenizer.from_pretrained("ferdaous/mytexttosqlmodel3")

prompt = "Write a SQL query that lists all employees who joined in the last 30 days."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))



import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_path='ferdaous/mytexttosqlmodel3'
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_path)

def generate_sql(prompt):
    formatted_prompt = f"### Instruction:\n{prompt}\n\n### SQL:\n"
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        do_sample=False,
    )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "### SQL:" in decoded:
        decoded = decoded.split("### SQL:")[1].strip()
    if "### Explanation:" in decoded:
        decoded = decoded.split("### Explanation:")[0].strip()
    return decoded

gr.Interface(fn=generate_sql,
             inputs=gr.Textbox(lines=4, label="Natural Language SQL Prompt"),
             outputs=gr.Textbox(label="Generated SQL"),
             title="LLaMA-3 SQL Generator",
             description="Enter a natural language question and get a SQL query in return!").launch()



